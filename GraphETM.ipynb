{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# GraphETM Dev Notebook",
   "id": "75852b8e51af2279"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T02:54:12.218172Z",
     "start_time": "2025-06-06T02:54:06.231964Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Imports\n",
    "## Local\n",
    "from model.graphetm_trainer import GraphETMTrainer\n",
    "\n",
    "## External\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Torch-Geometric\n",
    "import torch_geometric as pyg\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch_geometric.utils as pyg_utils\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import NeighborLoader\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Plot\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import wandb\n",
    "\n",
    "### Parameters\n",
    "wandb.login()"
   ],
   "id": "b594b9c64bb75b73",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mloicduch\u001B[0m (\u001B[33mloicduch-mcgill-university\u001B[0m) to \u001B[32mhttps://api.wandb.ai\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T02:54:12.232542Z",
     "start_time": "2025-06-06T02:54:12.220231Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Seeds\n",
    "pyg.seed_everything(10) # random, np, torch, torch.cuda"
   ],
   "id": "557db418d1a5cca5",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Model"
   ],
   "id": "a67e9036b54022b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T02:54:12.243614Z",
     "start_time": "2025-06-06T02:54:12.233998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "        Encoder module for GraphETM.\n",
    "\n",
    "        Attributes:\n",
    "                q_theta: q_theta\n",
    "                theta_act: theta_act\n",
    "                mu_q_theta: mu_q_theta\n",
    "                logsigma_q_theta: logsigma_q_theta\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_topics: int,\n",
    "            vocab_size: int,\n",
    "            encoder_hidden_size: int,\n",
    "            dropout: float = 0.5,\n",
    "            theta_act: str = 'tanh'\n",
    "    ):\n",
    "        \"\"\"\n",
    "            Initialize the Encoder module.\n",
    "\n",
    "            Args:\n",
    "                num_topics: Number of topics.\n",
    "                vocab_size: Size of vocabulary.\n",
    "                encoder_hidden_size: Size of the hidden layer in the encoder.\n",
    "                theta_act: Activation function for theta.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Dropout\n",
    "        self.thres_dropout = dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Theta Activation\n",
    "        self.theta_act = self._get_activation(theta_act)\n",
    "\n",
    "        ## define variational distribution for \\theta_{1:D} via amortization\n",
    "        self.q_theta = nn.Sequential(\n",
    "            nn.Linear(vocab_size, encoder_hidden_size),\n",
    "            self.theta_act,\n",
    "            nn.Linear(encoder_hidden_size, 64),\n",
    "            self.theta_act,\n",
    "        )\n",
    "        self.mu_q_theta = nn.Linear(64, num_topics, bias=True)\n",
    "        self.logsigma_q_theta = nn.Linear(64, num_topics, bias=True)\n",
    "\n",
    "    def infer_topic_distribution(self, normalized_bows: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "            Returns a deterministic topic distribution for evaluation purposes bypassing the stochastic reparameterization step.\n",
    "\n",
    "            Args:\n",
    "                normalized_bows (torch.Tensor): Normalized bag-of-words input.\n",
    "\n",
    "            Returns:\n",
    "                torch.Tensor: Deterministic topic proportions.\n",
    "        \"\"\"\n",
    "        q_theta = self.q_theta(normalized_bows)\n",
    "        mu_theta = self.mu_q_theta(q_theta)\n",
    "        theta = F.softmax(mu_theta, dim=-1)\n",
    "        return theta\n",
    "\n",
    "    def forward(self, bow_norm: torch.Tensor, free_nat: int = 0.5):\n",
    "        \"\"\"\n",
    "        Returns parameters of the variational distribution for \\theta.\n",
    "\n",
    "        Args:\n",
    "            bow_norm: (batch, V) Normalized batch of Bag-of-Words.\n",
    "\n",
    "        Returns:\n",
    "            mu_theta: mu_theta\n",
    "            logsigma_theta: logsigma_theta\n",
    "            kl_theta: kl_theta\n",
    "\n",
    "        \"\"\"\n",
    "        q_theta = self.q_theta(bow_norm)\n",
    "        if self.thres_dropout > 0:\n",
    "            q_theta = self.dropout(q_theta)\n",
    "        mu_theta = self.mu_q_theta(q_theta)\n",
    "        logsigma_theta = self.logsigma_q_theta(q_theta)\n",
    "\n",
    "        # KL[q(theta)||p(theta)] = lnq(theta) - lnp(theta)\n",
    "        kl_theta = -0.5 * torch.sum(1 + logsigma_theta - mu_theta.pow(2) - logsigma_theta.exp(), dim=1).mean()\n",
    "        # Free-bits\n",
    "        # kl_theta = torch.clamp(kl_theta, min=free_nat).mean()\n",
    "\n",
    "        return mu_theta, logsigma_theta, kl_theta\n",
    "\n",
    "    def _get_activation(self, act): # TODO: Redundant method.\n",
    "        if act == 'tanh':\n",
    "            act = nn.Tanh()\n",
    "        elif act == 'relu':\n",
    "            act = nn.ReLU()\n",
    "        elif act == 'softplus':\n",
    "            act = nn.Softplus()\n",
    "        elif act == 'rrelu':\n",
    "            act = nn.RReLU()\n",
    "        elif act == 'leakyrelu':\n",
    "            act = nn.LeakyReLU()\n",
    "        elif act == 'elu':\n",
    "            act = nn.ELU()\n",
    "        elif act == 'selu':\n",
    "            act = nn.SELU()\n",
    "        elif act == 'glu':\n",
    "            act = nn.GLU()\n",
    "        else:\n",
    "            print('Defaulting to tanh activations...')\n",
    "            act = nn.Tanh()\n",
    "        return act"
   ],
   "id": "2ee6a634b98f14cf",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T02:54:12.249867Z",
     "start_time": "2025-06-06T02:54:12.245335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "        Decoder module for GraphETM.\n",
    "\n",
    "        Attributes:\n",
    "            rho: Word embedding matrix.\n",
    "            alphas: Topic embedding matrix.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_dim: int,\n",
    "            num_topics: int,\n",
    "    ):\n",
    "        \"\"\"\n",
    "            Initialize the Decoder module.\n",
    "\n",
    "            Args:\n",
    "                num_topics: Number of topics.\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Objective: The latent topic distribution theta for (scRNA and EHR) are multiplied with Beta (essentially grounding the latent topics with the knowledge).\n",
    "\n",
    "        ## define the word embedding matrix \\rho\n",
    "        self.rho = None # V x L\n",
    "\n",
    "        ## define the matrix containing the topic embeddings\n",
    "        self.alphas = nn.Linear(in_dim, num_topics, bias=False)\n",
    "\n",
    "    def get_beta(self):\n",
    "        \"\"\"\n",
    "            Retrieve beta by doing softmax over the vocabulary dimension.\n",
    "\n",
    "            Returns:\n",
    "                Beta which represents the topic-word (or topic-feature) distributions.\n",
    "        \"\"\"\n",
    "        logits = self.alphas(self.rho).T\n",
    "        beta = F.softmax(logits, dim=1)\n",
    "        return beta\n",
    "\n",
    "    def forward(self, theta, rho):\n",
    "        self.rho = rho # Update embeddings\n",
    "\n",
    "        beta = self.get_beta()\n",
    "        preds = torch.log(torch.mm(theta, beta) + 1e-8)\n",
    "        return preds"
   ],
   "id": "7432107750b5f322",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T02:54:12.255930Z",
     "start_time": "2025-06-06T02:54:12.250931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class GraphFilter(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_dim: int,\n",
    "            hidden_dim: int,\n",
    "            out_dim: int,\n",
    "            edge_index: torch.LongTensor,\n",
    "    ):\n",
    "        super(GraphFilter, self).__init__()\n",
    "\n",
    "        self.edge_index = edge_index\n",
    "\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, embedding: torch.Tensor, batch=None):\n",
    "        # Batch\n",
    "        batch_ids = batch.n_id\n",
    "        embedding_batch = embedding[batch_ids]\n",
    "\n",
    "        # Forward\n",
    "        x = self.conv1(embedding_batch, batch.edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, batch.edge_index) # [N, out_dim]\n",
    "        # TODO: Normalize/dropout potentially to be added.\n",
    "\n",
    "        # Loss\n",
    "        loss = self.loss(x, batch) # Graph Reconstruction Loss # DONE: (NEXT) adapt loss function for mini batch\n",
    "        return x, loss # TODO: Return batch_ids? (sub_nodes)\n",
    "\n",
    "    # Graph Reconstruction Loss\n",
    "    def loss(self, x: torch.Tensor, batch):\n",
    "        # Positive edges\n",
    "        pos_edge_index = batch.edge_index  # shape [2, E_pos] # TODO: Get edge_index for batch instead (batch.edge_index)\n",
    "\n",
    "        # Negative edges\n",
    "        neg_edge_index = pyg_utils.negative_sampling(\n",
    "            edge_index=pos_edge_index,\n",
    "            num_nodes=x.size(0),\n",
    "            num_neg_samples=pos_edge_index.size(1))\n",
    "\n",
    "        # Gather embeddings\n",
    "        src_pos = x[pos_edge_index[0]] # [E_pos, out_dim]\n",
    "        dst_pos = x[pos_edge_index[1]] # [E_pos, out_dim]\n",
    "        src_neg = x[neg_edge_index[0]] # [E_pos, out_dim]\n",
    "        dst_neg = x[neg_edge_index[1]] # [E_pos, out_dim]\n",
    "\n",
    "        # Inner-product score\n",
    "        pos_scores = (src_pos * dst_pos).sum(dim=1)\n",
    "        neg_scores = (src_neg * dst_neg).sum(dim=1)\n",
    "\n",
    "        # Compute loss\n",
    "        pos_loss = F.binary_cross_entropy_with_logits(pos_scores, torch.ones_like(pos_scores))\n",
    "        neg_loss = F.binary_cross_entropy_with_logits(neg_scores, torch.ones_like(neg_scores))\n",
    "        return pos_loss + neg_loss"
   ],
   "id": "9624582a2a336f9e",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T02:54:12.267634Z",
     "start_time": "2025-06-06T02:54:12.257326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            encoder_params  : Dict[str, Dict[str, int]],\n",
    "            graphconv_params: Dict,\n",
    "            theta_act: str,\n",
    "            num_topics: int,\n",
    "            embedding: torch.Tensor = None,\n",
    "            embedding_dataloader = None,\n",
    "            edge_index: torch.LongTensor = None,\n",
    "            id_embed_sc : np.ndarray = None,\n",
    "            id_embed_ehr: np.ndarray = None,\n",
    "            trainable_embeddings = False, # Must be false to keep embeddings stable.\n",
    "            dropout = 0.2\n",
    "    ):\n",
    "        \"\"\"\n",
    "            Initialize the ETM model.\n",
    "\n",
    "            Args:\n",
    "                encoder_params: Dictionary of the parameters for the encoders. Dictionary {'sc': {str: Any}, 'ehr': {str: Any}}.\n",
    "                    vocab_size: Size of vocabulary.\n",
    "                    encoder_hidden_size: Size of the hidden layer in the encoder.\n",
    "                theta_act: Activation function for theta.\n",
    "                num_topics: Number of topics.\n",
    "\n",
    "                embedding: Initial embedding (also known as rho) computed from the knowledge graph (e.g.: TransE embeddings).\n",
    "                id_embed_sc : Index map for the genes found in the Gene Expression (BoW) matrix input and the Knowledge Graph embedding genes. It should be a numpy list where each index maps to a gene in the embeddings. This allows aligning the relevant genes to the genes found in the embeddings.\n",
    "                id_embed_ehr: Index map for the diseases found in the Electronic Health Record (BoW) matrix input and the Knowledge Graph embedding diseases. It should be a numpy list where each index maps to a disease in the embeddings. This allows aligning the relevant genes to the genes found in the embeddings.\n",
    "                trainable_embeddings: Whether to fine-tune word embeddings.\n",
    "\n",
    "                dropout: Dropout rate.\n",
    "\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.encoder_params = encoder_params\n",
    "\n",
    "        self.rho = nn.Parameter(embedding.clone(), requires_grad=trainable_embeddings) # V x L\n",
    "        self.edge_index = edge_index\n",
    "        self.id_embed_sc  = torch.tensor(id_embed_sc , dtype=torch.long)\n",
    "        self.id_embed_ehr = torch.tensor(id_embed_ehr, dtype=torch.long)\n",
    "\n",
    "        self.embedding_dataloader = embedding_dataloader\n",
    "\n",
    "        in_dim = embedding.shape[1]\n",
    "        self.graph_filter = GraphFilter(**graphconv_params, in_dim=in_dim, edge_index=self.edge_index)\n",
    "\n",
    "        self.enc_sc  = Encoder(**encoder_params['sc'],  num_topics=num_topics, dropout=dropout, theta_act=theta_act)\n",
    "        self.enc_ehr = Encoder(**encoder_params['ehr'], num_topics=num_topics, dropout=dropout, theta_act=theta_act)\n",
    "        self.dec_sc  = Decoder(in_dim=len(id_embed_sc) , num_topics=num_topics)\n",
    "        self.dec_ehr = Decoder(in_dim=len(id_embed_ehr), num_topics=num_topics)\n",
    "\n",
    "    # theta ~ mu + std N(0,1)\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        \"\"\"\n",
    "            Returns a sample from a Gaussian distribution via reparameterization.\n",
    "        \"\"\"\n",
    "        if self.training:\n",
    "            std = torch.exp(0.5 * logvar)\n",
    "            eps = torch.randn_like(std)\n",
    "            return eps.mul_(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def infer_topic_distribution(self, normalized_bows: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "            Returns a deterministic topic distribution for evaluation purposes bypassing the stochastic reparameterization step.\n",
    "\n",
    "            Args:\n",
    "                normalized_bows (torch.Tensor): Normalized bag-of-words input.\n",
    "\n",
    "            Returns:\n",
    "                torch.Tensor: Deterministic topic proportions.\n",
    "        \"\"\"\n",
    "        theta = self.encoder.infer_topic_distribution(normalized_bows) # TODO: To fix.\n",
    "        return theta\n",
    "\n",
    "    def get_beta(self, modality: str):\n",
    "        \"\"\"\n",
    "            Retrieve beta for the selecting modality which represents the topic-word (or topic-feature) distributions for that modality. It performs softmax of the vocabulary dimension. Calling this method puts the model into an evaluation state.\n",
    "\n",
    "            Args:\n",
    "                modality (str): \"sc\" single-cell RNA modality or \"ehr\" Electronic Health Record (diseases) modality.\n",
    "\n",
    "            Returns:\n",
    "                 np.ndarray: Beta representing the topic-word (or topic-feature) distributions.\n",
    "        \"\"\"\n",
    "        if modality == 'sc':\n",
    "            decoder = self.dec_sc\n",
    "        elif modality == 'ehr':\n",
    "            decoder = self.dec_ehr\n",
    "        else:\n",
    "            raise ValueError('The modality parameter must be either \"sc\" or \"ehr\".')\n",
    "\n",
    "        with torch.no_grad():\n",
    "            beta = decoder.get_beta().cpu().numpy()\n",
    "        return beta\n",
    "\n",
    "    def step_forward(self, encoder, decoder, bow, rho, aggregate=True):\n",
    "        bow_raw  = bow # integer counts\n",
    "        lengths  = bow_raw.sum(1, keepdim=True) + 1e-8\n",
    "        bow_norm = bow_raw / lengths # Normalize\n",
    "\n",
    "        mu, logvar, kld = encoder(bow_norm)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        theta = F.softmax(z, dim=-1)\n",
    "\n",
    "        preds = decoder(theta, rho=rho)\n",
    "        rec_loss = -(bow_raw * preds).sum(1) / lengths.squeeze(1) # Dev. note: lengths.squeeze(1) is the only key difference.\n",
    "        if aggregate:\n",
    "            rec_loss = rec_loss.mean()\n",
    "\n",
    "        return {\n",
    "            'rec_loss': rec_loss,\n",
    "            'kl'      : kld,\n",
    "            'theta'   : theta.detach(),\n",
    "            'preds'   : preds.detach(),\n",
    "        }\n",
    "\n",
    "    def forward(self, bow_sc, bow_ehr, kl_annealing=1.0): # TODO: Update forward updated trainable graph layer.\n",
    "        # Update Embeddings\n",
    "        if self.embedding_dataloader is not None:\n",
    "            graph_loss = 0\n",
    "            for embed_batch in self.embedding_dataloader:\n",
    "                rho, batch_graph_loss = self.graph_filter(self.rho, embed_batch)\n",
    "                graph_loss += batch_graph_loss\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "            # rho, graph_loss = self.graph_filter(self.rho) # DONE: Add graph (batch?) as param? Since I cant really run it inside this forward method now. Unless I write an alternative method just for it.\n",
    "            # TODO: Re-implement the who codebase (make trainer GraphETM?)\n",
    "\n",
    "        rho_sc  = rho[self.id_embed_sc , :]\n",
    "        rho_ehr = rho[self.id_embed_ehr, :]\n",
    "\n",
    "        # Encoder-Decoder: ScRNA\n",
    "        output_sc = self.step_forward(\n",
    "            bow=bow_sc,\n",
    "            encoder=self.enc_sc,\n",
    "            decoder=self.dec_sc,\n",
    "            rho=rho_sc)\n",
    "\n",
    "        # Encoder-Decoder: EHR\n",
    "        output_ehr = self.step_forward(\n",
    "            bow=bow_ehr,\n",
    "            encoder=self.enc_ehr,\n",
    "            decoder=self.dec_ehr,\n",
    "            rho=rho_ehr)\n",
    "\n",
    "        # Total ELBO Loss\n",
    "        elbo_loss = (output_sc['rec_loss'] + output_ehr['rec_loss']).mean() + (output_sc['kl'] + output_ehr['kl']) * kl_annealing + graph_loss\n",
    "\n",
    "        # Update outputs\n",
    "        output_sc['rec_loss'] = output_sc['rec_loss'].mean()\n",
    "        output_ehr['rec_loss'] = output_ehr['rec_loss'].mean()\n",
    "        return {\n",
    "            'loss': elbo_loss,\n",
    "            'graph_loss': graph_loss,\n",
    "            'sc' : output_sc,\n",
    "            'ehr': output_ehr,\n",
    "        }"
   ],
   "id": "7cb68b25dc80d861",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Training"
   ],
   "id": "dc5337a766dbb8e5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T02:54:13.980260Z",
     "start_time": "2025-06-06T02:54:13.949114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.mps.is_available() else 'cpu')\n",
    "# device = torch.device('cpu')\n",
    "print(f'Using device: {device}')"
   ],
   "id": "253bea9b7ac31024",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T02:54:17.484849Z",
     "start_time": "2025-06-06T02:54:16.607117Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Data\n",
    "test_size = 0.2\n",
    "\n",
    "# Load metadata\n",
    "input_sc = pd.read_csv('inputs/GraphETM/input_PBMC.csv')\n",
    "input_ehr = pd.read_csv('inputs/GraphETM/input_EHR.csv')\n",
    "\n",
    "# Load Rho (Graph Embeddings)\n",
    "sc_indices  = np.load('inputs/GraphETM/id_embed_sc.npy')\n",
    "ehr_indices = np.load('inputs/GraphETM/id_embed_ehr.npy')\n",
    "\n",
    "embedding_full = torch.load('inputs/GraphETM/embedding_full.pt', weights_only=False)\n",
    "edge_index = torch.load('inputs/GraphETM/edge_index.pt', weights_only=True)\n",
    "\n",
    "embedding_data = Data(x=embedding_full, edge_index=edge_index)\n",
    "embedding_dataloader = NeighborLoader(\n",
    "    data=embedding_data,\n",
    "    num_neighbors=[10, 5],\n",
    "    batch_size=1024,\n",
    ")\n",
    "\n",
    "# Load Input Data\n",
    "X_sc  = torch.load('inputs/GraphETM/X_sc.pt',  weights_only=False)\n",
    "X_ehr = torch.load('inputs/GraphETM/X_ehr.pt', weights_only=False)\n",
    "\n",
    "X_sc,  X_sc_val  = train_test_split(X_sc , test_size=test_size, random_state=0)\n",
    "X_ehr, X_ehr_val = train_test_split(X_ehr, test_size=test_size, random_state=0)"
   ],
   "id": "e72ba919c7a2536a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T02:54:41.352478Z",
     "start_time": "2025-06-06T02:54:39.275525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Parameters\n",
    "config = {\n",
    "    'model': dict(\n",
    "        num_topics = 15, # K = 10\n",
    "        theta_act='tanh',\n",
    "        dropout=0.2,\n",
    "        encoder_params = { ## Encoder parameters\n",
    "            'sc': { # Encoder SC\n",
    "                'vocab_size': X_sc.shape[1],\n",
    "                'encoder_hidden_size': 64 },\n",
    "            'ehr': { # Encoder EHR\n",
    "                'vocab_size': X_ehr.shape[1],\n",
    "                'encoder_hidden_size': 64 }\n",
    "        },\n",
    "        graphconv_params = { ## Graph-Conv Filter parameters\n",
    "            'hidden_dim': X_ehr.shape[1],\n",
    "            'out_dim': 64\n",
    "        },\n",
    "        ## Embedding Parameters\n",
    "        embedding  = embedding_full,\n",
    "        edge_index = edge_index,\n",
    "        id_embed_sc  = sc_indices ,\n",
    "        id_embed_ehr = ehr_indices,\n",
    "    ),\n",
    "\n",
    "    'dataloader': dict(\n",
    "        batch_size=64,\n",
    "        shuffle=False\n",
    "    ),\n",
    "\n",
    "    'training': dict(\n",
    "        optimizer = torch.optim.Adam,\n",
    "        lr = 0.001,\n",
    "        epochs = 100,\n",
    "        kl_annealing_epochs = None\n",
    "    ),\n",
    "\n",
    "    'device': device,\n",
    "}\n",
    "\n",
    "\n",
    "### Model\n",
    "trainer = GraphETMTrainer(\n",
    "    model = Model(**config['model']).to(device), # Model\n",
    "    dataloader_sc  = DataLoader(**config['dataloader'], dataset = X_sc ), # Dataloaders\n",
    "    dataloader_ehr = DataLoader(**config['dataloader'], dataset = X_ehr),\n",
    "    val_dataloader_sc  = DataLoader(**config['dataloader'], dataset = X_sc_val ),\n",
    "    val_dataloader_ehr = DataLoader(**config['dataloader'], dataset = X_ehr_val),\n",
    "    device = device,\n",
    "    wandb_run = wandb.init(\n",
    "        project ='GraphETM',\n",
    "        group = 'GraphETM',\n",
    "        # name = f'GraphETM_{int(time.time())}',\n",
    "        name = 'GraphETM_conv_filter',\n",
    "        config=config, save_code=True) # Start Wandb\n",
    ")\n",
    "\n",
    "### Training\n",
    "trainer.train( # TODO: Check for KL Divergence being magnitude smaller than Recon. Loss.\n",
    "    epochs = config['training']['epochs'],\n",
    "    optimizer = config['training']['optimizer']([ # Optimizer\n",
    "        {'params': trainer.model.enc_sc.parameters()},\n",
    "        {'params': trainer.model.enc_ehr.parameters()},\n",
    "        {'params': trainer.model.dec_sc.parameters()},\n",
    "        {'params': trainer.model.dec_ehr.parameters()}\n",
    "        # Embedding params: lr=0 until un-frozen\n",
    "        # {'params': [trainer.model.dec_sc.embedding],  'lr': 0.0, 'name': 'embedding_sc' },\n",
    "        # {'params': [trainer.model.dec_ehr.embedding], 'lr': 0.0, 'name': 'embedding_ehr'},\n",
    "    ],\n",
    "    lr = config['training']['lr']),\n",
    "    kl_annealing_epochs = config['training']['kl_annealing_epochs'],\n",
    ")\n",
    "# TODO: Close the trainer.wandb instance."
   ],
   "id": "5976a6a312ce2e02",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>/Users/loicduchesne/Library/CloudStorage/OneDrive-Personal/Projects/Graph-scETM/wandb/run-20250605_225439-qixn6wk4</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/loicduch-mcgill-university/GraphETM/runs/qixn6wk4' target=\"_blank\">GraphETM_conv_filter</a></strong> to <a href='https://wandb.ai/loicduch-mcgill-university/GraphETM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/loicduch-mcgill-university/GraphETM' target=\"_blank\">https://wandb.ai/loicduch-mcgill-university/GraphETM</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/loicduch-mcgill-university/GraphETM/runs/qixn6wk4' target=\"_blank\">https://wandb.ai/loicduch-mcgill-university/GraphETM/runs/qixn6wk4</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Training GraphETM:   0%|          | 0/100 [00:00<?, ?epoch/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "17e020c418194d4f901336ca2b40009f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Invalid buffer size: 39.21 GB",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 59\u001B[0m\n\u001B[1;32m     43\u001B[0m trainer \u001B[38;5;241m=\u001B[39m GraphETMTrainer(\n\u001B[1;32m     44\u001B[0m     model \u001B[38;5;241m=\u001B[39m Model(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m'\u001B[39m])\u001B[38;5;241m.\u001B[39mto(device), \u001B[38;5;66;03m# Model\u001B[39;00m\n\u001B[1;32m     45\u001B[0m     dataloader_sc  \u001B[38;5;241m=\u001B[39m DataLoader(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mconfig[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdataloader\u001B[39m\u001B[38;5;124m'\u001B[39m], dataset \u001B[38;5;241m=\u001B[39m X_sc ), \u001B[38;5;66;03m# Dataloaders\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     55\u001B[0m         config\u001B[38;5;241m=\u001B[39mconfig, save_code\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m) \u001B[38;5;66;03m# Start Wandb\u001B[39;00m\n\u001B[1;32m     56\u001B[0m )\n\u001B[1;32m     58\u001B[0m \u001B[38;5;66;03m### Training\u001B[39;00m\n\u001B[0;32m---> 59\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m# TODO: Check for KL Divergence being magnitude smaller than Recon. Loss.\u001B[39;49;00m\n\u001B[1;32m     60\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtraining\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mepochs\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     61\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtraining\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43moptimizer\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;66;43;03m# Optimizer\u001B[39;49;00m\n\u001B[1;32m     62\u001B[0m \u001B[43m        \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mparams\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menc_sc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     63\u001B[0m \u001B[43m        \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mparams\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menc_ehr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     64\u001B[0m \u001B[43m        \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mparams\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdec_sc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     65\u001B[0m \u001B[43m        \u001B[49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mparams\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdec_ehr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mparameters\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m}\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# Embedding params: lr=0 until un-frozen\u001B[39;49;00m\n\u001B[1;32m     67\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# {'params': [trainer.model.dec_sc.embedding],  'lr': 0.0, 'name': 'embedding_sc' },\u001B[39;49;00m\n\u001B[1;32m     68\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# {'params': [trainer.model.dec_ehr.embedding], 'lr': 0.0, 'name': 'embedding_ehr'},\u001B[39;49;00m\n\u001B[1;32m     69\u001B[0m \u001B[43m    \u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     70\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlr\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtraining\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mlr\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     71\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkl_annealing_epochs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtraining\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mkl_annealing_epochs\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     72\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;66;03m# TODO: Close the trainer.wandb instance.\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-Personal/Projects/Graph-scETM/model/graphetm_trainer.py:135\u001B[0m, in \u001B[0;36mGraphETMTrainer.train\u001B[0;34m(self, epochs, optimizer, kl_annealing_epochs, verbose)\u001B[0m\n\u001B[1;32m    132\u001B[0m bow_sc, bow_ehr \u001B[38;5;241m=\u001B[39m bow_sc\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice), bow_ehr\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m    133\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m--> 135\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbow_sc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbow_sc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbow_ehr\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbow_ehr\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkl_annealing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mkl_annealing\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    137\u001B[0m loss \u001B[38;5;241m=\u001B[39m output[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m    138\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "Cell \u001B[0;32mIn[6], line 119\u001B[0m, in \u001B[0;36mModel.forward\u001B[0;34m(self, bow_sc, bow_ehr, kl_annealing)\u001B[0m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, bow_sc, bow_ehr, kl_annealing\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1.0\u001B[39m): \u001B[38;5;66;03m# TODO: Update forward updated trainable graph layer.\u001B[39;00m\n\u001B[1;32m    118\u001B[0m     \u001B[38;5;66;03m# Update Embeddings\u001B[39;00m\n\u001B[0;32m--> 119\u001B[0m     rho, graph_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgraph_filter\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrho\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    121\u001B[0m     rho_sc  \u001B[38;5;241m=\u001B[39m rho[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mid_embed_sc , :]\n\u001B[1;32m    122\u001B[0m     rho_ehr \u001B[38;5;241m=\u001B[39m rho[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mid_embed_ehr, :]\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-Personal/Projects/Graph-scETM/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-Personal/Projects/Graph-scETM/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[0;32mIn[5], line 17\u001B[0m, in \u001B[0;36mGraphFilter.forward\u001B[0;34m(self, embedding)\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, embedding: torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[0;32m---> 17\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv1\u001B[49m\u001B[43m(\u001B[49m\u001B[43membedding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43medge_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m     x \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mrelu(x)\n\u001B[1;32m     19\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv2(x, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39medge_index) \u001B[38;5;66;03m# [N, out_dim]\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-Personal/Projects/Graph-scETM/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1749\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1750\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1751\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-Personal/Projects/Graph-scETM/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1757\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1758\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1759\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1760\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1761\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1762\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1764\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1765\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-Personal/Projects/Graph-scETM/.venv/lib/python3.10/site-packages/torch_geometric/nn/conv/gcn_conv.py:263\u001B[0m, in \u001B[0;36mGCNConv.forward\u001B[0;34m(self, x, edge_index, edge_weight)\u001B[0m\n\u001B[1;32m    260\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlin(x)\n\u001B[1;32m    262\u001B[0m \u001B[38;5;66;03m# propagate_type: (x: Tensor, edge_weight: OptTensor)\u001B[39;00m\n\u001B[0;32m--> 263\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpropagate\u001B[49m\u001B[43m(\u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43medge_weight\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    265\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    266\u001B[0m     out \u001B[38;5;241m=\u001B[39m out \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbias\n",
      "File \u001B[0;32m/var/folders/zx/8f4kdwmj1bn4vsw4b_62tln40000gn/T/torch_geometric.nn.conv.gcn_conv_GCNConv_propagate_62jgctmu.py:183\u001B[0m, in \u001B[0;36mpropagate\u001B[0;34m(self, edge_index, x, edge_weight, size)\u001B[0m\n\u001B[1;32m    177\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupdate(\n\u001B[1;32m    178\u001B[0m         out,\n\u001B[1;32m    179\u001B[0m     )\n\u001B[1;32m    181\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 183\u001B[0m     kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcollect\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    184\u001B[0m \u001B[43m        \u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    185\u001B[0m \u001B[43m        \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    186\u001B[0m \u001B[43m        \u001B[49m\u001B[43medge_weight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    187\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmutable_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    188\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    190\u001B[0m     \u001B[38;5;66;03m# Begin Message Forward Pre Hook #######################################\u001B[39;00m\n\u001B[1;32m    191\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39mis_scripting() \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_compiling():\n",
      "File \u001B[0;32m/var/folders/zx/8f4kdwmj1bn4vsw4b_62tln40000gn/T/torch_geometric.nn.conv.gcn_conv_GCNConv_propagate_62jgctmu.py:96\u001B[0m, in \u001B[0;36mcollect\u001B[0;34m(self, edge_index, x, edge_weight, size)\u001B[0m\n\u001B[1;32m     94\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x, Tensor):\n\u001B[1;32m     95\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_set_size(size, j, x)\n\u001B[0;32m---> 96\u001B[0m     x_j \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_index_select\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_index_j\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     97\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     98\u001B[0m     x_j \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-Personal/Projects/Graph-scETM/.venv/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py:267\u001B[0m, in \u001B[0;36mMessagePassing._index_select\u001B[0;34m(self, src, index)\u001B[0m\n\u001B[1;32m    265\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m src\u001B[38;5;241m.\u001B[39mindex_select(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnode_dim, index)\n\u001B[1;32m    266\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 267\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_index_select_safe\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-Personal/Projects/Graph-scETM/.venv/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py:290\u001B[0m, in \u001B[0;36mMessagePassing._index_select_safe\u001B[0;34m(self, src, index)\u001B[0m\n\u001B[1;32m    281\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (index\u001B[38;5;241m.\u001B[39mnumel() \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m index\u001B[38;5;241m.\u001B[39mmax() \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m src\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnode_dim)):\n\u001B[1;32m    282\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIndexError\u001B[39;00m(\n\u001B[1;32m    283\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound indices in \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124medge_index\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m that are larger \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    284\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthan \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msrc\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnode_dim)\u001B[38;5;250m \u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;241m1\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m (got \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    287\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124min the interval [0, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msrc\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnode_dim)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m) in \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    288\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124myour node feature matrix and try again.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 290\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "File \u001B[0;32m~/Library/CloudStorage/OneDrive-Personal/Projects/Graph-scETM/.venv/lib/python3.10/site-packages/torch_geometric/nn/conv/message_passing.py:271\u001B[0m, in \u001B[0;36mMessagePassing._index_select_safe\u001B[0;34m(self, src, index)\u001B[0m\n\u001B[1;32m    269\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_index_select_safe\u001B[39m(\u001B[38;5;28mself\u001B[39m, src: Tensor, index: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m    270\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 271\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43msrc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex_select\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnode_dim\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    272\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m (\u001B[38;5;167;01mIndexError\u001B[39;00m, \u001B[38;5;167;01mRuntimeError\u001B[39;00m) \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    273\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m index\u001B[38;5;241m.\u001B[39mnumel() \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m index\u001B[38;5;241m.\u001B[39mmin() \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Invalid buffer size: 39.21 GB"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### FUNCTION TOP_K\n",
    "TOP_K = 5\n",
    "\n",
    "# TODO REVERSE RANKING + CORRESPONDING\n",
    "\n",
    "def top_k_per_topic(input_df, modality, k=5):\n",
    "    beta = trainer.model.get_beta(modality=modality)\n",
    "\n",
    "    top_k_indices = np.argsort(beta, axis=1)[:, -k:]\n",
    "    top_k_indices = top_k_indices.flatten()\n",
    "    top_k = input_df.columns[top_k_indices]\n",
    "\n",
    "    prob = beta[:, top_k_indices].T\n",
    "    return pd.DataFrame(prob, index=top_k)\n",
    "\n",
    "### GET TOP K PER TOPIC\n",
    "sc_prob_df = top_k_per_topic(input_df=input_sc, modality='sc', k=TOP_K) # SC\n",
    "ehr_prob_df = top_k_per_topic(input_df=input_ehr, modality='ehr', k=TOP_K) # EHR\n",
    "\n",
    "### PLOT\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    horizontal_spacing=0.06,\n",
    "    subplot_titles=['Top Gene per Topics', 'Top ICD-9 Code per Topics'],\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    template='plotly_white',\n",
    "    width=1200, height=1500,\n",
    "    font=dict(color='black', size=10),\n",
    ")\n",
    "\n",
    "# PARAMS\n",
    "heatmap_params = dict(\n",
    "    colorscale='OrRd',\n",
    "    xgap=0.9,\n",
    "    ygap=0.9,\n",
    ")\n",
    "\n",
    "yaxes_params = dict(\n",
    "    tickfont=dict(size=10, color='black')\n",
    ")\n",
    "\n",
    "# SC Plot\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        name='SC',\n",
    "        z=sc_prob_df.values,\n",
    "        x=sc_prob_df.columns,\n",
    "        y=list(range(len(sc_prob_df.index))),\n",
    "        **heatmap_params\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.update_yaxes(\n",
    "    tickvals=list(range(len(sc_prob_df.index))),\n",
    "    ticktext=sc_prob_df.index,\n",
    "    autorange='reversed', type='category',\n",
    "    row=1, col=1,\n",
    "    **yaxes_params\n",
    ")\n",
    "\n",
    "# EHR Plot\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        name='EHR',\n",
    "        z=ehr_prob_df.values,\n",
    "        x=ehr_prob_df.columns,\n",
    "        y=list(range(len(ehr_prob_df.index))),\n",
    "        **heatmap_params\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "fig.update_yaxes(\n",
    "    tickvals=list(range(len(ehr_prob_df.index))),\n",
    "    ticktext=ehr_prob_df.index,\n",
    "    autorange='reversed', type='category',\n",
    "    row=1, col=2,\n",
    "    **yaxes_params\n",
    ")\n",
    "\n",
    "# Horizontal separations\n",
    "for i in range(TOP_K, ehr_prob_df.shape[0], TOP_K):\n",
    "    fig.add_hline(\n",
    "        y = i - 0.5,\n",
    "        line_width=4,\n",
    "        line_color='white'\n",
    "    )\n",
    "\n",
    "# Adjust vertical title location\n",
    "for annotation in fig['layout']['annotations']:\n",
    "    annotation['y'] += 0.01\n",
    "\n",
    "fig.show()"
   ],
   "id": "60ae96bb3a94f8fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# fig.write_html('top_k.html')",
   "id": "1cd5be9d713bec01",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "beta_sc = trainer.model.dec_sc.get_beta()      # K  4340\n",
    "beta_ehr = trainer.model.dec_ehr.get_beta()      # K  4340\n",
    "\n",
    "uniq_top1_sc = np.unique(beta_sc.numpy(force=True).argmax(1)).size\n",
    "uniq_top1_ehr = np.unique(beta_ehr.numpy(force=True).argmax(1)).size\n",
    "print(f'unique top-1 tokens: sc = {uniq_top1_sc}/{beta_sc.shape[0]}, ehr = {uniq_top1_ehr}/{beta_ehr.shape[0]}')\n",
    "\n",
    "entropy_sc = -(beta_sc * beta_sc.clamp_min(1e-9).log()).sum(1)\n",
    "entropy_ehr = -(beta_ehr * beta_ehr.clamp_min(1e-9).log()).sum(1)\n",
    "print(f'entropy per topic: sc = {entropy_sc.numpy(force=True)}, ehr = {entropy_ehr.numpy(force=True)}')"
   ],
   "id": "7a72936d778800a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### OCCURRENCE COUNT\n",
    "TOP_N = 25\n",
    "\n",
    "gene_counts = sc_prob_df.index.value_counts()\n",
    "icd_counts  = ehr_prob_df.index.value_counts()\n",
    "\n",
    "gene_counts_top = gene_counts.head(TOP_N)\n",
    "icd_counts_top  = icd_counts.head(TOP_N)\n",
    "\n",
    "# fig_num_topic = make_subplots(\n",
    "#     rows=1, cols=2,\n",
    "#     shared_xaxes=False,\n",
    "#     # horizontal_spacing=0.06,\n",
    "#     subplot_titles=[f'Top {TOP_N} genes by num_topics (K={K})', f'Top {TOP_N} ICD-9 codes by num_topics (K={K})']\n",
    "# )\n",
    "#\n",
    "# fig_num_topic.update_layout(\n",
    "#     template='plotly_white',\n",
    "#     font=dict(color='black', size=10)\n",
    "# )\n",
    "#\n",
    "# fig_num_topic.add_bar() # TODO: Got lazy.\n",
    "\n",
    "fig_gene_count = px.bar(\n",
    "    gene_counts_top.sort_values(ascending=False).reset_index(),\n",
    "    x='index', y='count',\n",
    "    title=f'Top {TOP_N} genes by num_topics (K={TOP_N})'\n",
    ")\n",
    "\n",
    "fig_icd_count = px.bar(\n",
    "    icd_counts_top.sort_values(ascending=False).reset_index(),\n",
    "    x='index', y='count',\n",
    "    title=f'Top {TOP_N} ICD-9 codes by num_topics (K={TOP_N})'\n",
    ")\n",
    "\n",
    "### PROBABILITY WEIGHTED IMPORTANCE\n",
    "gene_weight = sc_prob_df.groupby(sc_prob_df.index).sum().sum(axis=1)\n",
    "icd_weight  = ehr_prob_df.groupby(ehr_prob_df.index).sum().sum(axis=1)\n",
    "\n",
    "gene_weight_top = gene_weight.sort_values(ascending=False).head(TOP_N)\n",
    "icd_weight_top  = icd_weight.sort_values(ascending=False).head(TOP_N)\n",
    "\n",
    "fig_gene_weight = px.bar(\n",
    "    gene_weight_top.reset_index(),\n",
    "    x='index', y=0,\n",
    "    title=f'Top {TOP_N} genes by cumulative beta-probability',\n",
    "    labels={'index':'Gene', 0:' '},\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "fig_icd_weight = px.bar(\n",
    "    icd_weight_top.reset_index(),\n",
    "    x='index', y=0,\n",
    "    title=f'Top {TOP_N} ICD-9 codes by cumulative beta-probability',\n",
    "    labels={'index':'ICD-9', 0:' '},\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "### FORMAT FIGURES\n",
    "font_params = dict(color='black', size=12)\n",
    "for fig in [fig_gene_count, fig_icd_count, fig_gene_weight, fig_icd_weight]:\n",
    "    fig.update_layout(\n",
    "        template='plotly_white',\n",
    "        font=font_params,\n",
    "        title_font=dict(color='black', size=16)\n",
    "    )\n",
    "    fig.update_xaxes(tickfont=font_params, title_font=dict(color='black', size=14))\n",
    "    fig.update_yaxes(tickfont=font_params, title_font=dict(color='black', size=14))\n",
    "\n",
    "fig_gene_count.show()\n",
    "fig_icd_count.show()\n",
    "fig_gene_weight.show()\n",
    "fig_icd_weight.show()\n",
    "\n",
    "\n",
    "############################################################################\n",
    "### CUMULATIVE VS UBIQUITY\n",
    "font_params = dict(color='black', size=12)\n",
    "\n",
    "fig_scatter = px.scatter(\n",
    "    data_frame = pd.DataFrame({\n",
    "        'term':  list(gene_counts.index) + list(icd_counts.index),\n",
    "        'num_topics':  gene_counts.tolist()    + icd_counts.tolist(),\n",
    "        'cum_beta': pd.concat([gene_weight, icd_weight]).values,\n",
    "        'type': ['Gene']*len(gene_counts) + ['ICD-9']*len(icd_counts)\n",
    "    })\n",
    "    .query('cum_beta > 0')\n",
    "    ,\n",
    "    x='num_topics', y='cum_beta',\n",
    "    color='type', # two colors = Genes vs ICD-9\n",
    "    hover_data=['term', 'num_topics', 'cum_beta'],\n",
    "    marginal_x='violin',\n",
    "    marginal_y='violin',\n",
    "    # log_y=True, # keeps long-tail terms visible\n",
    "    template='plotly_white',\n",
    "    title='Term ubiquity vs cumulative probability',\n",
    ")\n",
    "\n",
    "# Update visuals\n",
    "fig_scatter.update_layout(\n",
    "    font=font_params,\n",
    "    title_font=dict(color='black', size=16),\n",
    "    legend_title_text='Term type',\n",
    ")\n",
    "fig_scatter.update_xaxes(title_font=font_params, tickfont=font_params,\n",
    "                         rangemode='tozero')\n",
    "fig_scatter.update_yaxes(title_font=font_params, tickfont=font_params,\n",
    "                         rangemode='tozero')\n",
    "\n",
    "fig_scatter.show()"
   ],
   "id": "9c8f247941818897",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# wandb.log({'Top K per Topics': fig}) # TODO: Fix visualization.\n",
    "\n",
    "wandb.log({\n",
    "    'Gene freq':        wandb.Plotly(fig_gene_count),\n",
    "    'ICD freq':         wandb.Plotly(fig_icd_count),\n",
    "    'Gene importance':  wandb.Plotly(fig_gene_weight),\n",
    "    'ICD importance':   wandb.Plotly(fig_icd_weight),\n",
    "})\n",
    "\n",
    "# fig_scatter.write_html('scatter.html', include_plotlyjs='cdn') # TODO: Fix visualization\n",
    "# scatter_artifact = wandb.Artifact('ubiquity_vs_importance', type='visualization')\n",
    "# scatter_artifact.add_file('scatter.html')\n",
    "# wandb.log_artifact(scatter_artifact)"
   ],
   "id": "d10ed407e463de73",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TODO: Implement Plotly Clustergram.",
   "id": "9dd4fd3dfc3f7ffd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-06T02:55:00.733315Z",
     "start_time": "2025-06-06T02:54:57.662688Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.wandb.finish()",
   "id": "381c123766894cba",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">GraphETM_conv_filter</strong> at: <a href='https://wandb.ai/loicduch-mcgill-university/GraphETM/runs/qixn6wk4' target=\"_blank\">https://wandb.ai/loicduch-mcgill-university/GraphETM/runs/qixn6wk4</a><br> View project at: <a href='https://wandb.ai/loicduch-mcgill-university/GraphETM' target=\"_blank\">https://wandb.ai/loicduch-mcgill-university/GraphETM</a><br>Synced 8 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>./wandb/run-20250605_225439-qixn6wk4/logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# DONE",
   "id": "1758aab5795437fe",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
